{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to request url\n",
    "def request_url(url):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f'Failed to fetch {url}')\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f'Timeout error: {url}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse html\n",
    "def parse_html(to_parse):\n",
    "    \"\"\"\n",
    "    It takes a string, then parse it.\n",
    "    Finally, it retuns a soup object.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(to_parse, 'html.parser')\n",
    "    return soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = set()\n",
    "\n",
    "# extracting data fron news section\n",
    "for i in range(1,234):\n",
    "    url = f'https://analyticsdrift.com/news/page/{i}/'\n",
    "    soup = parse_html(request_url(url))\n",
    "    data = soup.find_all('h3', class_ = 'entry-title td-module-title')\n",
    "    for item in data:\n",
    "        link = item.a.get('href')\n",
    "        full_link = f'News {link}'\n",
    "        all_urls.add(full_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # extracting data from Data Science section\n",
    "for i in range(1, 29):\n",
    "    url = f'https://analyticsdrift.com/data-science/page/{i}/'\n",
    "    soup = parse_html(request_url(url))\n",
    "    data = soup.find_all('h3', class_ = 'entry-title td-module-title')\n",
    "    for item in data:\n",
    "        link = item.a.get('href')\n",
    "        full_link = f'Datascience {link}'\n",
    "        all_urls.add(full_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from Developer section\n",
    "for i in range(1, 6):\n",
    "    url = f'https://analyticsdrift.com/developer/page/{i}/'\n",
    "    soup = parse_html(request_url(url))\n",
    "    data = soup.find_all('h3', class_ = 'entry-title td-module-title')\n",
    "    for item in data:\n",
    "        link = item.a.get('href')\n",
    "        full_link = f'Developer {link}'\n",
    "        all_urls.add(full_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from miscellaneous section\n",
    "for i in range(1, 13):\n",
    "    url = f'https://analyticsdrift.com/miscellaneous/page/{i}/'\n",
    "    soup = parse_html(request_url(url))\n",
    "    data = soup.find_all('h3', class_ = 'entry-title td-module-title')\n",
    "    for item in data:\n",
    "        link = item.a.get('href')\n",
    "        full_link = f'Miscellaneous {link}'\n",
    "        all_urls.add(full_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358\n"
     ]
    }
   ],
   "source": [
    "# converting set to list because iteration not possible on set\n",
    "print(len(all_urls))\n",
    "all_urls = list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch article heading\n",
    "def fetch_article_heading(url):\n",
    "    try:\n",
    "        html_text = request_url(url)\n",
    "        if html_text:\n",
    "            soup = parse_html(html_text)\n",
    "            heading_element = soup.find('h1', class_='tdb-title-text')\n",
    "            if heading_element:\n",
    "                return heading_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or parsing heading from {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch article date\n",
    "def fetch_article_date(url):\n",
    "    try:\n",
    "        html_text = request_url(url)\n",
    "        if html_text:\n",
    "            soup = parse_html(html_text)\n",
    "            time_tag = soup.find('time', class_='entry-date updated td-module-date')\n",
    "            if time_tag:\n",
    "                return time_tag['datetime'].split('T')[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or parsing date from {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch article author\n",
    "def fetch_article_author(url):\n",
    "    try:\n",
    "        html_text = request_url(url)\n",
    "        if html_text:\n",
    "            soup = parse_html(html_text)\n",
    "            author_tag = soup.find('a', class_='tdb-author-name')\n",
    "            if author_tag:\n",
    "                return author_tag.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or parsing author from {url}: {e}\")\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extrach all necessary links\n",
    "def extract_links(url):\n",
    "    base_url = 'https://analyticsdrift.com/'\n",
    "    external_links = set()\n",
    "    internal_links = set()\n",
    "    broken_links = set()\n",
    "    new_tab_links = set()\n",
    "    same_tab_links = set()\n",
    "    sponsored_links = set()\n",
    "\n",
    "    html_text = request_url(url)\n",
    "    if html_text:\n",
    "        soup = parse_html(html_text)\n",
    "        data = soup.find_all('div', class_='tdb-block-inner td-fix-index')\n",
    "        \n",
    "        for div in data:\n",
    "            paragraphs = div.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                links = paragraph.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    if href:\n",
    "                        if base_url in href:\n",
    "                            internal_links.add(href)\n",
    "                        else:\n",
    "                            external_links.add(href)\n",
    "\n",
    "                        try:\n",
    "                            response = requests.head(href)\n",
    "                            if response.status_code != 200:\n",
    "                                broken_links.add(href)\n",
    "                                # If broken link found, exit the loop\n",
    "                                break\n",
    "                            else:\n",
    "                                content_type = response.headers.get('Content-Type', '').lower()\n",
    "                                if 'text' in content_type or 'html' in content_type:\n",
    "                                    inner_html_text = requests.get(href).text\n",
    "                                    inner_soup = parse_html(inner_html_text)\n",
    "                                    if inner_soup.find('a', target='_blank'):\n",
    "                                        new_tab_links.add(href)\n",
    "                                    else:\n",
    "                                        same_tab_links.add(href)\n",
    "                                    if inner_soup.find('a', rel='nofollow'):\n",
    "                                        sponsored_links.add(href)\n",
    "                                else:\n",
    "                                    print(f\"Skipping binary content for {href}\")\n",
    "\n",
    "                        except requests.RequestException as e:\n",
    "                            print(f\"HTTP request error for {href}: {e}\")\n",
    "                            broken_links.add(href)\n",
    "\n",
    "    return {\n",
    "        'external_link': \",\".join(external_links),\n",
    "        'internal_link': \",\".join(internal_links),\n",
    "        'broken_link': \",\".join(broken_links),\n",
    "        'new_tab_link': \",\".join(new_tab_links),\n",
    "        'same_tab_link': \",\".join(same_tab_links),\n",
    "        'sponsored_link': \",\".join(sponsored_links)\n",
    "    }\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_heading = []\n",
    "article_url = [] \n",
    "article_date = []\n",
    "external_links = []\n",
    "internal_links = []\n",
    "broken_links = []\n",
    "new_tab_links = []\n",
    "same_tab_links = []\n",
    "sponsored_links = []\n",
    "section_name = []\n",
    "author_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links number 200 and remaining 3158\n",
      "Extracted links number 201 and remaining 3157\n"
     ]
    }
   ],
   "source": [
    "count = 200 \n",
    "for url in all_urls[200 : 400]:\n",
    "    section_name.append(url.split(' ')[0])\n",
    "    article_url.append(url.split(' ')[1])\n",
    "    article_heading.append(fetch_article_heading(url.split(' ')[1]))\n",
    "    article_date.append(fetch_article_date(url.split(' ')[1]))\n",
    "    author_name.append(fetch_article_author(url.split(' ')[1]))\n",
    "    \n",
    "    links_info = extract_links(url.split(' ')[1])\n",
    "    external_links.append(links_info['external_link'])\n",
    "    internal_links.append(links_info['internal_link'])\n",
    "    broken_links.append(links_info['broken_link'])\n",
    "    new_tab_links.append(links_info['new_tab_link'])\n",
    "    same_tab_links.append(links_info['same_tab_link'])\n",
    "    sponsored_links.append(links_info['sponsored_link'])\n",
    "    print(f\"Extracted links number {count} and remaining {len(all_urls) - count}\")\n",
    "    count +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(article_heading))\n",
    "print(len(article_url))\n",
    "print(len(article_date))\n",
    "print(len(external_links))\n",
    "print(len(internal_links))\n",
    "print(len(broken_links))\n",
    "print(len(new_tab_links))\n",
    "print(len(same_tab_links))\n",
    "print(len(sponsored_links))\n",
    "print(len(section_name))\n",
    "print(len(author_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Article Heading': article_heading,\n",
    "    'Article Url': article_url,\n",
    "    'Article Section': section_name,\n",
    "    'Article Date': article_date,\n",
    "    'Author Name': author_name,\n",
    "    'External Links': external_links,\n",
    "    'Lnternal Llinks': internal_links,\n",
    "    'Broken Links': broken_links,\n",
    "    'Links Open in new tab': new_tab_links,\n",
    "    'Links open in same tab' : same_tab_links,\n",
    "    'Sponsored_Links': sponsored_links\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Heading</th>\n",
       "      <th>Article Url</th>\n",
       "      <th>Article Section</th>\n",
       "      <th>Article Date</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>External Links</th>\n",
       "      <th>Lnternal Llinks</th>\n",
       "      <th>Broken Links</th>\n",
       "      <th>Links Open in new tab</th>\n",
       "      <th>Links open in same tab</th>\n",
       "      <th>Sponsored_Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple’s Ali Farhadi Appointed CEO of Allen Ins...</td>\n",
       "      <td>https://analyticsdrift.com/apples-ali-farhadi-...</td>\n",
       "      <td>News</td>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>Sahil Pawar</td>\n",
       "      <td>https://allenai.org/</td>\n",
       "      <td>https://analyticsdrift.com/tag/artificial-inte...</td>\n",
       "      <td></td>\n",
       "      <td>https://analyticsdrift.com/microsoft-announces...</td>\n",
       "      <td>https://allenai.org/,https://analyticsdrift.co...</td>\n",
       "      <td>https://analyticsdrift.com/microsoft-announces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gupshup acquires Conversational AI provider As...</td>\n",
       "      <td>https://analyticsdrift.com/gupshup-acquires-co...</td>\n",
       "      <td>News</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>Dipayan Mitra</td>\n",
       "      <td>https://www.gupshup.io/resources/press-release...</td>\n",
       "      <td>https://analyticsdrift.com/south-korea-telecom...</td>\n",
       "      <td>https://www.gupshup.io/resources/press-release...</td>\n",
       "      <td>https://analyticsdrift.com/south-korea-telecom...</td>\n",
       "      <td></td>\n",
       "      <td>https://analyticsdrift.com/south-korea-telecom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon launches Free AWS Builder Online Series...</td>\n",
       "      <td>https://analyticsdrift.com/amazon-launches-fre...</td>\n",
       "      <td>News</td>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>Dipayan Mitra</td>\n",
       "      <td>https://aws.amazon.com/events/builders-online-...</td>\n",
       "      <td>https://analyticsdrift.com/meta-develops-ai-th...</td>\n",
       "      <td></td>\n",
       "      <td>https://analyticsdrift.com/meta-develops-ai-th...</td>\n",
       "      <td></td>\n",
       "      <td>https://analyticsdrift.com/meta-develops-ai-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elon Musk might Start his own Social Media wit...</td>\n",
       "      <td>https://analyticsdrift.com/elon-musk-might-sta...</td>\n",
       "      <td>News</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Dipayan Mitra</td>\n",
       "      <td>https://t.co/aPS9ycji37</td>\n",
       "      <td>https://analyticsdrift.com/tesla-to-accept-dog...</td>\n",
       "      <td>https://t.co/aPS9ycji37</td>\n",
       "      <td>https://analyticsdrift.com/tesla-to-accept-dog...</td>\n",
       "      <td></td>\n",
       "      <td>https://analyticsdrift.com/tesla-to-accept-dog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Article Heading  \\\n",
       "0  Apple’s Ali Farhadi Appointed CEO of Allen Ins...   \n",
       "1  Gupshup acquires Conversational AI provider As...   \n",
       "2  Amazon launches Free AWS Builder Online Series...   \n",
       "3  Elon Musk might Start his own Social Media wit...   \n",
       "\n",
       "                                         Article Url Article Section  \\\n",
       "0  https://analyticsdrift.com/apples-ali-farhadi-...            News   \n",
       "1  https://analyticsdrift.com/gupshup-acquires-co...            News   \n",
       "2  https://analyticsdrift.com/amazon-launches-fre...            News   \n",
       "3  https://analyticsdrift.com/elon-musk-might-sta...            News   \n",
       "\n",
       "  Article Date    Author Name  \\\n",
       "0   2023-06-21    Sahil Pawar   \n",
       "1   2022-04-21  Dipayan Mitra   \n",
       "2   2021-12-23  Dipayan Mitra   \n",
       "3   2022-04-01  Dipayan Mitra   \n",
       "\n",
       "                                      External Links  \\\n",
       "0                               https://allenai.org/   \n",
       "1  https://www.gupshup.io/resources/press-release...   \n",
       "2  https://aws.amazon.com/events/builders-online-...   \n",
       "3                            https://t.co/aPS9ycji37   \n",
       "\n",
       "                                     Lnternal Llinks  \\\n",
       "0  https://analyticsdrift.com/tag/artificial-inte...   \n",
       "1  https://analyticsdrift.com/south-korea-telecom...   \n",
       "2  https://analyticsdrift.com/meta-develops-ai-th...   \n",
       "3  https://analyticsdrift.com/tesla-to-accept-dog...   \n",
       "\n",
       "                                        Broken Links  \\\n",
       "0                                                      \n",
       "1  https://www.gupshup.io/resources/press-release...   \n",
       "2                                                      \n",
       "3                            https://t.co/aPS9ycji37   \n",
       "\n",
       "                               Links Open in new tab  \\\n",
       "0  https://analyticsdrift.com/microsoft-announces...   \n",
       "1  https://analyticsdrift.com/south-korea-telecom...   \n",
       "2  https://analyticsdrift.com/meta-develops-ai-th...   \n",
       "3  https://analyticsdrift.com/tesla-to-accept-dog...   \n",
       "\n",
       "                              Links open in same tab  \\\n",
       "0  https://allenai.org/,https://analyticsdrift.co...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "\n",
       "                                     Sponsored_Links  \n",
       "0  https://analyticsdrift.com/microsoft-announces...  \n",
       "1  https://analyticsdrift.com/south-korea-telecom...  \n",
       "2  https://analyticsdrift.com/meta-develops-ai-th...  \n",
       "3  https://analyticsdrift.com/tesla-to-accept-dog...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analyticsdrift.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
