{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics drift website url\n",
    "url = 'https://analyticsdrift.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_url(link):     \n",
    "    time.sleep(3) \n",
    "    response = requests.get(link, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {link}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse the html\n",
    "def parse_html(to_parse):\n",
    "    \"\"\"\n",
    "    It takes a string, then parse it.\n",
    "    Finally, it retuns a soup object.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(to_parse, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_sections(main_url):\n",
    "   soup = parse_html(request_url(url))\n",
    "   section_list = []\n",
    "   ul = soup.find('div' , class_ = \"td_block_inner td-fix-index\")\n",
    "   for li in ul.find_all('li'):\n",
    "       section_list.append(li.a.get('href'))\n",
    "   # Remove the section which we will not consider\n",
    "   # Like Services , Contact us, About us....\n",
    "   section_list = section_list[1:5]    \n",
    "   return section_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://analyticsdrift.com/news/',\n",
       " 'https://analyticsdrift.com/data-science/',\n",
       " 'https://analyticsdrift.com/developer/',\n",
       " 'https://analyticsdrift.com/miscellaneous/']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the sections url\n",
    "all_section = all_sections(url)\n",
    "all_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for valid urls\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        urllib.request.urlopen(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every section has multiple pages with a different number of pages, so we have to extract article links for each section separately.\n",
    "\n",
    "News Section: This section has 233 pages.\n",
    "\n",
    "Data Science Section: This section has 28 pages.\n",
    "\n",
    "Developer Section: This section has 5 pages.\n",
    "\n",
    "Miscellaneous Section: This section has 12 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_section(section_url, num_pages):\n",
    "    all_urls = set()\n",
    "    for i in range(1, num_pages + 1):\n",
    "        page_url = f\"{section_url}/page/{i}/\" \n",
    "        html = request_url(page_url)\n",
    "        if html:\n",
    "            soup = parse_html(html)\n",
    "            data = soup.find_all('div', class_='td-module-meta-info')\n",
    "            for item in data:\n",
    "                link = item.a.get('href')\n",
    "                if link and is_valid_url(link):\n",
    "                    all_urls.add(link)\n",
    "        else:\n",
    "            break\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_urls():\n",
    "    all_urls = set()\n",
    "\n",
    "    # Extracting article URLs from News Section\n",
    "    news_section_url = f\"{url}/news\"\n",
    "    all_urls.update(extract_urls_from_section(news_section_url, 1))\n",
    "\n",
    "    # Extracting article URLs from Data Science Section\n",
    "    data_science_section_url = f\"{url}/data-science\"\n",
    "    all_urls.update(extract_urls_from_section(data_science_section_url, 1))\n",
    "\n",
    "    # Extracting article URLs from Developer Section\n",
    "    developer_section_url = f\"{url}/developer\"\n",
    "    all_urls.update(extract_urls_from_section(developer_section_url, 1))\n",
    "\n",
    "    # Extracting article URLs from Miscellaneous Section\n",
    "    miscellaneous_section_url = f\"{url}/miscellaneous\"\n",
    "    all_urls.update(extract_urls_from_section(miscellaneous_section_url, 1))\n",
    "\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch URL: https://analyticsdrift.com/mistral-ais-new-llm-model-outperforms-gpt-3-model//news/page/1/\n",
      "Failed to fetch URL: https://analyticsdrift.com/mistral-ais-new-llm-model-outperforms-gpt-3-model//data-science/page/1/\n",
      "Failed to fetch URL: https://analyticsdrift.com/mistral-ais-new-llm-model-outperforms-gpt-3-model//developer/page/1/\n",
      "Failed to fetch URL: https://analyticsdrift.com/mistral-ais-new-llm-model-outperforms-gpt-3-model//miscellaneous/page/1/\n"
     ]
    }
   ],
   "source": [
    "all_url = extract_article_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracring article heading from each article\n",
    "def article_heading(article_url):\n",
    "    soup = parse_html(request_url(article_url))\n",
    "    article_heading = soup.find('h1', class_='tdb-title-text').text.strip()\n",
    "    return article_heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all the artcle heading in a list \n",
    "article_headings = []\n",
    "for url in all_url:\n",
    "    article_headings.append(article_heading(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(article_headings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting internal links\n",
    "def extract_internal_links(page_url):\n",
    "    internal_links = set()  \n",
    "\n",
    "    # Fetching the HTML content of the page\n",
    "    soup = parse_html(request_url(page_url))\n",
    "\n",
    "    # Find all <a> tags with href attribute\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "\n",
    "        # Skip empty links\n",
    "        if not link:\n",
    "            continue\n",
    "\n",
    "        # Check if the link is internal\n",
    "        if link.startswith('http://') or link.startswith('https://'):\n",
    "            if link.startswith('https://analyticsdrift.com/'):\n",
    "                internal_links.add(link)  \n",
    "\n",
    "    return list(internal_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting external links \n",
    "def extract_external_links(page_url):\n",
    "    external_links = set()  \n",
    "\n",
    "    # Fetching the HTML content of the page\n",
    "    soup = parse_html(request_url(page_url))\n",
    "\n",
    "    # Find all <a> tags with href attribute\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "\n",
    "        # Skip empty links\n",
    "        if not link:\n",
    "            continue\n",
    "\n",
    "        # Check if the link is external\n",
    "        if link.startswith('http://') or link.startswith('https://'):\n",
    "            if not link.startswith('https://analyticsdrift.com/'):\n",
    "                external_links.add(link)  \n",
    "\n",
    "    return list(external_links)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# storing all the external links and internal links into the list\n",
    "external_links =[]\n",
    "internal_links = []\n",
    "\n",
    "for url in all_url:\n",
    "    external_links.append(extract_external_links(url))\n",
    "    internal_links.append(extract_internal_links(url))\n",
    "\n",
    "print(len(external_links))\n",
    "print(len(internal_links)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract broken links from the articles\n",
    "def find_broken_links(page_url):\n",
    "    broken_links = []\n",
    "\n",
    "    # Fetching the HTML content of the page\n",
    "    soup = parse_html(request_url(page_url))    \n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "\n",
    "        # Skip empty links\n",
    "        if not link:\n",
    "            continue\n",
    "\n",
    "        # Check if the link is reachable\n",
    "        try:\n",
    "            response = requests.head(link)\n",
    "            if response.status_code != 200:\n",
    "                broken_links.append(link)\n",
    "        except Exception as e:\n",
    "            # If any exception occurs, consider the link broken\n",
    "            broken_links.append(link)\n",
    "    \n",
    "    if len(broken_links)==0:\n",
    "        broken_links.append(None)        \n",
    "\n",
    "    return broken_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all the broken link into a  list variable\n",
    "broken_links = []\n",
    "for url in all_url:\n",
    "    broken_links.append(find_broken_links(url)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# checking length of the broken link list variable\n",
    "print(len(broken_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_new_tabs(page_url):\n",
    "    not_new_tab = set()\n",
    "    soup = parse_html(request_url(page_url))\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # Loop through each link and check its attributes\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        target = link.get('target')\n",
    "        \n",
    "        # Check if the link has a target attribute set to \"_blank\"\n",
    "        if target == '_blank':\n",
    "            not_new_tab.add(f\"Link: {href} opens in a new tab\")\n",
    "        else:\n",
    "            not_new_tab.add(f\"Link: {href} does not open in a new tab\")\n",
    "\n",
    "    return list(not_new_tab)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_new_tab = []\n",
    "for url in all_url:\n",
    "    not_new_tab.append(not_new_tabs(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "#checking length of the vaiable\n",
    "print(len(not_new_tab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to find sponsored links\n",
    "def find_sponserd_link(page_url):\n",
    "    sponserd_link = set()\n",
    "    soup = parse_html(request_url(page_url))\n",
    "    \n",
    "    for a_tag in soup.find_all('a', rel='sponsored'):\n",
    "        link = a_tag['href']\n",
    "        sponserd_link.add(link)\n",
    "    \n",
    "    if len(sponserd_link)==0:\n",
    "        sponserd_link.add(None)\n",
    "    \n",
    "    return list(sponserd_link)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all sponserd links in a list variable\n",
    "sponserd_links = []\n",
    "for url in all_url:\n",
    "    sponserd_links.append(find_sponserd_link(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(sponserd_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_no = [i for i in range(len(all_url))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createing dictionary to store all data\n",
    "\n",
    "data = {\n",
    "    'SL No' : sl_no,    \n",
    "    'Title' : article_headings,\n",
    "    'External Link' : external_links,\n",
    "    'Internal Link:' : internal_links,\n",
    "    'Broken Links' : broken_links,\n",
    "    'Not New Tab' : not_new_tab,\n",
    "    'Sponsored' : sponserd_links\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating dataframe using dictionary variable\n",
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('SL No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>External Link</th>\n",
       "      <th>Internal Link:</th>\n",
       "      <th>Broken Links</th>\n",
       "      <th>Not New Tab</th>\n",
       "      <th>Sponsored</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SL No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weis Wave: Revolutionizing Market Analysis</td>\n",
       "      <td>[https://api.whatsapp.com/send?text=Weis+Wave%...</td>\n",
       "      <td>[https://analyticsdrift.com/developer/, https:...</td>\n",
       "      <td>[#, #, #login-form, https://www.linkedin.com/c...</td>\n",
       "      <td>[Link: https://analyticsdrift.com/wp-content/u...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI Open Sources Triton 1.0, A GPU Programm...</td>\n",
       "      <td>[https://analyticsdrift.com, https://twitter.c...</td>\n",
       "      <td>[https://analyticsdrift.com/weis-wave-revoluti...</td>\n",
       "      <td>[#, #, #login-form, https://www.linkedin.com/c...</td>\n",
       "      <td>[Link: /openai-open-sources-triton-1-0-a-gpu-p...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optimus Gen-2, Second Generation Humanoid Robo...</td>\n",
       "      <td>[https://discord.com/invite/GkKUqKZYaG, https:...</td>\n",
       "      <td>[https://analyticsdrift.com/is-grok-the-first-...</td>\n",
       "      <td>[#, #, #login-form, https://www.linkedin.com/c...</td>\n",
       "      <td>[Link: https://analyticsdrift.com/swaayatt-rob...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Navigating UK Healthcare in the Digital Age</td>\n",
       "      <td>[https://discord.com/invite/GkKUqKZYaG, https:...</td>\n",
       "      <td>[https://analyticsdrift.com/weis-wave-revoluti...</td>\n",
       "      <td>[#, #, #login-form, https://www.linkedin.com/c...</td>\n",
       "      <td>[Link: https://analyticsdrift.com/author/ratan...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Microsoft Unveils Phi-2, a Small Language Mode...</td>\n",
       "      <td>[https://discord.com/invite/GkKUqKZYaG, https:...</td>\n",
       "      <td>[https://analyticsdrift.com/meta-unveils-open-...</td>\n",
       "      <td>[#, #, #login-form, https://www.linkedin.com/c...</td>\n",
       "      <td>[Link: https://analyticsdrift.com/swaayatt-rob...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "SL No                                                      \n",
       "0             Weis Wave: Revolutionizing Market Analysis   \n",
       "1      OpenAI Open Sources Triton 1.0, A GPU Programm...   \n",
       "2      Optimus Gen-2, Second Generation Humanoid Robo...   \n",
       "3            Navigating UK Healthcare in the Digital Age   \n",
       "4      Microsoft Unveils Phi-2, a Small Language Mode...   \n",
       "\n",
       "                                           External Link  \\\n",
       "SL No                                                      \n",
       "0      [https://api.whatsapp.com/send?text=Weis+Wave%...   \n",
       "1      [https://analyticsdrift.com, https://twitter.c...   \n",
       "2      [https://discord.com/invite/GkKUqKZYaG, https:...   \n",
       "3      [https://discord.com/invite/GkKUqKZYaG, https:...   \n",
       "4      [https://discord.com/invite/GkKUqKZYaG, https:...   \n",
       "\n",
       "                                          Internal Link:  \\\n",
       "SL No                                                      \n",
       "0      [https://analyticsdrift.com/developer/, https:...   \n",
       "1      [https://analyticsdrift.com/weis-wave-revoluti...   \n",
       "2      [https://analyticsdrift.com/is-grok-the-first-...   \n",
       "3      [https://analyticsdrift.com/weis-wave-revoluti...   \n",
       "4      [https://analyticsdrift.com/meta-unveils-open-...   \n",
       "\n",
       "                                            Broken Links  \\\n",
       "SL No                                                      \n",
       "0      [#, #, #login-form, https://www.linkedin.com/c...   \n",
       "1      [#, #, #login-form, https://www.linkedin.com/c...   \n",
       "2      [#, #, #login-form, https://www.linkedin.com/c...   \n",
       "3      [#, #, #login-form, https://www.linkedin.com/c...   \n",
       "4      [#, #, #login-form, https://www.linkedin.com/c...   \n",
       "\n",
       "                                             Not New Tab Sponsored  \n",
       "SL No                                                               \n",
       "0      [Link: https://analyticsdrift.com/wp-content/u...    [None]  \n",
       "1      [Link: /openai-open-sources-triton-1-0-a-gpu-p...    [None]  \n",
       "2      [Link: https://analyticsdrift.com/swaayatt-rob...    [None]  \n",
       "3      [Link: https://analyticsdrift.com/author/ratan...    [None]  \n",
       "4      [Link: https://analyticsdrift.com/swaayatt-rob...    [None]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
